[ { "title": "Detecção de objetos com haarcascade", "url": "/posts/cap5/", "categories": "haarcascade, detecção, classificador", "tags": "", "date": "2020-12-04 08:25:34 -0300", "snippet": "Detecção de objetos usando o método Haar CascadeNesse capítulo você irá aprender uma maneira rápida e direta de como criar um classificador Haar Cascade. Além disso, no final do capítulo será disponibilizado um código para detecção de objetos com o classificador criado, que com pequenas alterações, pode se adequar a qualquer situação.Nesse contexto, o método Haar Cascade, é um método de detecção de objetos proposto por Paul Viola e Michael Jones. É uma abordagem baseada em Machine Learning, em que uma função cascade é treinada com muitas imagens positivas e negativas. Logo, é usado para detectar objetos em outras imagens.Preparando o ambientePara esse projeto é necessário que você tenha instalado em sua máquina apenas 3 itens.1 - Editor de textos de sua preferência, eu particularmente uso o Visual Studio Code.2 - Alguma versão Python de sua preferência, eu particularmente uso a 3.8.5.3 - Biblioteca OpenCV .Aqui não irei explicar como você pode fazer o download e instalação desses itens, pois na internet existem diversos tutorias detalhados de como fazer isso.PassosA criação de um classficador usando o HaarCascade pode ser descrita em um conjunto de 5 passos.1 - Escolher o objeto.2 - Selecionar imagens negativas.3 - Selecionar imagens positivas.4 - Gerar o vetor de positivas.5 - Treinar o classificador.1 - Escolher o objeto.O primeiro passo é escolher o objeto que será identificado, para isso você deverá pensar nos seguintes aspectos:* Serão objetos rígidos como uma logo (nike) ou com variações (cadeira,copo)?* Objetos rigidos são mais eficientes e mais rápidos.* Ao treinar muitas variações pode ser que o classificador fique fraco, portanto, fique atento a isso.* Objetos que a cor é fundamental não são recomendados, pois as imagens serão passadas para a escala de cinza.Para esse projeto, escolhi o objeto faca para ser detectado.2 - Selecionar imagens negativas.Para selecionar as imagens negativas, você deve ficar atento aos seguintes aspectos:* Podem ser qualquer coisa, menos o objeto.* Devem ser maiores que as positivas, pois a openCV vai colocar as imagens positivas dentro das negativas.* Se possível usar fotos de prováveis fundos onde o objeto é encontrado. *Ex: Objeto = carro Usar imagens de asfalto e ruas vazias.Logo, você deve ficar atento as imagens escolhidas, pois como dito elas podem ser qualquer coisa, exceto o objeto escolhido, como escolhemos facas como objeto de detecção devemos, iremos precisar de imagens que não tenham facas.Quantas imagens negativas?É relativo, para esse projeto eu conto com 3000 imagens negativas, com diversas variações de fundo. Entretanto, isso vai depender dos resultados obtidos no treinamento, pode ser que eu precise de mais imagens ou não, isso será explicado mais a frente com mais detalhes.Exemplos de imagens negativas: Figura 1 Figura 2 Figura 3OBS: Todas essas imagens tem dimensões 100x100, essa informação será importante para futuras explicações.Aqui é importante mencionar que você deve criar uma pasta (ex: projeto) onde estará outra pasta com as imagens negativas, na pasta projeto também deve estar as imagens positivas.Exemplo: PastaNa pasta das imagens negativas você deve colocar esse arquivo:https://github.com/luis131313/cookbook/blob/master/imagens/cap2/criar_lista.batE deve executá-lo ao final da escolha das imagens negativas, pois ele vai gerar uma lista com as imagens negativas.3 -Selecionar imagens positivas.Para selecionar as imagens positivas, você deve ficar atento aos seguintes aspectos:* Apenas o objeto.* Quantas imagens? * Depende da: Qualidade da imagem, tipo do objeto, poder computacional disponível.* As imagens devem ter o mesmo tamanho e a proporção precisa ser a mesma, caso contrário a openCV faz isso automaticamente e gera problemas de distorção do objeto. * Ex: Uma imagem 100x50, passada pra 25x25, vai ter o objeto descaracterizado.* Imagens grandes podem gerar problemas, fazendo o treinamento durar até meses.* Sempre que possível usar imagens com fundo branco.Como dito no primeiro passo, você deve tomar cuidado com as variações do objeto, caso você queira realizar a detecção de um objeto em diferentes ângulos é sugerido que você faça diferentes classificadores. Para exemplificar isso, cito o classificador haarcascade frontal face, que realiza a detecção frontal da face (esse classificador inclusive é de fácil acesso, até a openCV disponibiliza ele pra você) e caso você queira a detecção lateral da face, você deve usar outro classificador, esses cuidados devem ser tomados para que você tenha um bom classificador.Em relação a quantidade, alguns estudos sugerem que um bom classificador deve ter no mínimo 5000 mil imagens como entrada para o treinamento.Exemplos de imagens positivas que irei usar para o treinamento do classificador: Figura 1 Figura 2 Figura 3OBS: Todas essas imagens tem dimensões 100x50, essa informação será importante para explicações futuras.Aqui temos o pulo do gato, é possível criar mais imagens positivas a partir das imagens que você já tem, para isso baixe esse arquivo:https://github.com/luis131313/cookbook/blob/master/imagens/cap2/opencv_createsamples.exeColoque ele junto com as imagens positivas, depois basta abrir o CMD, entrar na pasta onde estão suas imagens positivas e digitar esse comando:opencv_createsamples -img faca_1.png -bg negativas/bg.txt -info positivas/positivas.lst -maxxangle 0.5 -maxyangle 0.5 -maxzangle 0.5 -num 300 -bgcolor 255 -bgthresh 10Parâmetros:-img = Nome da imagem base.-bg = Nome da pasta / nome do arquivo .txt com as informações das imagens negativas.-info = Nome da pasta / Nome do arquivo .lst (sempre altere esse parâmetro quando usar uma nova imagem (Ex: positivas2/positivas2.lst, positivas3/positivas3.lst)).-maxangle (x,y,z) = Variação de rotação que a imagem terá.-num = Número de imagens que serão criadas.-bgtresh = parâmetro que permite a retirada do fundo da imagem, deixando apenas o objeto de interesse (aqui se justifica o fundo branco). Esse parâmetro deve ser analisado com cuidado, pois: bgtresh 10 bgtresh 100Como resultado você terá a quantidade de imagens mencionada em uma pasta de acordo com o nome que você escolheu e um arquivo .lst que terá informações sobre essas imagens, esse arquivo é de extrema importância e é a partir dele que iremos criar o vetor de imagens.Nesse caso, usei 10 imagens e criei um total de 3000 imagens a partir desse comando, logo, você terá as pastas positivas1, positivas2 e etc…Ex: Pasta4 - Gerar o vetor de positivas.Aqui você deverá criar um vetor para cada pasta com as imagens positiva (positivas1, positivas2, etc…) e depois juntar esses vetores em apenas um vetor.Para isso, digite esse comando no CMD:opencv_createsamples -info positivas1/positivas1.lst -num 2000 -w 50 -h 25 -vec vetor1.vecParâmetros:-info = Nome da pasta que contém as imagens / arquivo .lst (Como criamos 3000 imagens a partir de 10 imagens, temos 10 pastas com 300 imagens cada, logo, teremos que repetir esse comando 10 vezes, alterando o nome da pasta e do arquivo .lst, para positivas2 / positivas2.lst, etc…). w e -h = são as dimensões, como nossas imagens eram 100 x 50, eu reduzi para 50 x 25, para reduzir o tamanho do arquivo, até porque para treinar o classificador com as imagens em 100 x 50 eu deveria ter um super computador. vec = Nome do vetor (Aqui você também tem que alterar, colocando vetor1, vetor2, etc…). Após isso, devemos unir todos esses vetores em apenas um, para isso crie uma pasta chamada “vec” e coloque todos os vetores nela.Depois baixe esse arquivo:https://github.com/luis131313/cookbook/blob/master/imagens/cap2/mergevec.pye coloque ele na pasta do seu projeto.após isso, digite no CMD:python mergevec.py -v vec/ -o vetor_final.vecApós a conclusão você terá um arquivo chamado vetor_final.vec, que é o vetor que iremos utilizar.Nesse momento a pasta do seu projeto estará assim: Pasta “Projeto”5 - Treinar o classificadorPara o passo final, você deve primeiramente baixar esses arquivos:https://github.com/luis131313/cookbook/blob/master/imagens/cap2/opencv_traincascade.exehttps://mega.nz/file/09YnVKQb#LdE1iz05i9OeoMqoZtuC3lVn4teeA7gqozVS-N1hG2UApós isso, você deve abrir a pasta negativas e colocar esses dois arquivos e o vetor final nela, e criar uma pasta chamada “classificador”.Dessa maneira: Pasta “classificador” ArquivosApós isso, abra o CMD na pasta negativas e digite o seguinte comando:opencv_traincascade -data classificador -vec vetor_final.vec -bg bg.txt -numPos 2400 -numNeg 1200 -numStages 15 -w 30 -h 15 -precalcBufSize 1024 -precalcIdxBufSize 1024Parâmetros-data = Nome da pasta que os arquivos de treinamento serão armazenados.-vec = Nome do vetor.-bg = informações das imagens negativas.-numPos = Número de imagens positivas.-numNeg = Número de imagens negativas.-numStages = Número de estágios.-w e -h = dimensões das imagens.-precalcBufSize e -precalcIdxBufSize = memória utilizada para o treinamento.Após o treinamento, na pasta classificador você terá esses arquivos: ArquivosO arquivo cascade.xml é o nosso classificador.O arquivo params.xml são os parâmetros usados no treinamento.E os outros arquivos, são os resultados de cada estágio do treinamento.Sobre o uso dos parâmetros: É indicado que você use metade do número de imagens positivas para as negativas no primeiro treinamento. Alguns estudos sugerem que um bom classificador deve ter no mínimo 5000 imagens positivas. Após o primeiro treinamento, se você notar que está tendo muitos falsos positivos, aumente o número de imagens negativas, se notar que não está realizando a detecção, aumente o número de imagens positivas, faça novos treinamentos até ter bons resultados. Para melhorar os resultados você também pode aumentar o número de estágios. Não se esqueça que a soma dos parâmetros -precalcBufSize e -precalcIdxBufSize não pode ser maior que a memória disponível. Quanto mais imagens negativas, positivas, estágios de treinamento e dimensão das imagens, mais o treinamento vai demorar, podendo fazer o treinamento durar até meses. Código para detecçãoAgora irei apresentar um código que irá realizar a detecção de facas.Para isso baixe esse classificador que eu criei, ele ainda não está pronto, logo não irá apresentar resultados excelentes.https://github.com/luis131313/cookbook/blob/master/imagens/cap2/cascade_facas.xmlimport cv2#variável que armazena a imagemimagem1 = &#39;teste1.png&#39;#variável que armazena o arquivo xmlcascade_path1 = &#39;cascade_facas.xml&#39; #cria o classificadorclf1 = cv2.CascadeClassifier(cascade_path1)#lê a imagemimg1 = cv2.imread(imagem1)#converte para cinzagray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)#Função da detecçãodeteccoes1 = clf1.detectMultiScale(gray1, scaleFactor=1.01, minNeighbors=5, minSize=(1,1))#desenha o retângulo com as coordenadas obtidasfor(x,y,w,h) in deteccoes1: img1 = cv2.rectangle(img1, (x,y), (x+w, y+h), (0,0,255), 2)#para visualizar a imagemcv2.imshow(&#39;Classificador 1&#39;, img1)#mantém a janela aberta até que eu digite uma teclacv2.waitKey(0)#destrói a janelacv2.destroyAllWindows()Ao executar o código com um exemplo, teremos essa detecção: Imagem retirada do Google Podemos notar alguns falsos positivos, o que indica que seria interessante realizar um novo treinamento com mais imagens negativas.Considerações finaisVários dos arquivos apresentados foram cedidos pela www.iaexpert.academy, agradeço imensamente pela generosidade.Fiz esse tutorial com muito carinho e espero que seja útil para você, a intenção aqui foi realizar um pequeno projeto usando o método HaarCascade, ainda existe muito há aprender sobre esse método, mas a minha intenção é apenas introduzir esse assunto.Desejo bons estudos e bons trabalhos.Atenciosamente,Luis Fernando Santos Ferreira, Aluno do curso de Ciência da Computação na Universidade Federal de Lavras.Linkedin: https://www.linkedin.com/in/luis-ferreira-3b02131a8/" }, { "title": "Filtro de densidade usando Machine Learning e Clusterização", "url": "/posts/cap3/", "categories": "agrupamento, densidade, dbscan", "tags": "", "date": "2020-12-04 08:25:34 -0300", "snippet": "Filtro de densidade usando Machine Learning e ClusterizaçãoNesse capítulo você irá aprender uma maneira rápida e fácil de utilizar Machine learning para remover ruídos em uma captura de objeto utilizando range de cores.Na captura de imagens utilizando cores, um dos grandes problemas encontrados é a remoção do ruído indesejado por objetos menores ao fundo do cenário.Nesse contexto, iremos utilizar o método DBSCAN da Lib SKLEARN para reorganizar os pixels das cores selecionadas e identificar áreas de baixa densidade.Obs. Os códigos contidos neste capítulo foram desenvolvidos para serem rodados no Google Colab e pode sofrer algumas alterações para rodar fora do Colab (exemplo: a utilização da lib google.colab.patches para visualizar imagens).Preparando o ambientePara esse projeto é necessário que você tenha instalado os seguintes itens em sua máquina. Editor de textos de sua preferência. Python 3.8.5 . Bibliotecas (OpenCV, Collections, Sklearn, Numpy, urllib, matplotlib). Caso queira, pode utilizar o Google Colab que tem o ambiente praticamente pronto.PassosImportar Libsimport numpy as npimport urllibimport cv2from google.colab.patches import cv2_imshowfrom collections import Counterfrom sklearn.cluster import DBSCANfrom sklearn import metricsfrom sklearn.datasets import make_blobsfrom sklearn.preprocessing import StandardScalerimport matplotlib.pyplot as pltDownload da imagemBaixamos a imagem diretamente da internet utilizando a lib Urlib e após isso, utilizamos as libs Numpy e Opencv para converter a imagem para um formato aceito pela lib OpenCV.Obs. Como estamos utilizando o Google Colab, estaremos realizando o download e conversão das imagens diretamente do Imgur.def url_to_image(url): resp = urllib.request.urlopen(url) image = np.asarray(bytearray(resp.read()), dtype=&quot;uint8&quot;) image = cv2.imdecode(image, cv2.IMREAD_COLOR) hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV) return hsvurl = f&#39;https://i.imgur.com/PLGlnWj.png&#39;img = url_to_image(url)Seleção do range de corEste é um dos momentos mais demorados e manuais da aplicação, pois teremos que manualmente encontrar o range de cores utilizados no objeto que queremos selecionar.No nosso caso é o Azul.img = url_to_image(url)BLUE_MIN = (110,50,50)BLUE_MAX = (130,255,255)Separação de pixelsNesta parte do código percorremos todos os pixels da imagem, verificamos quais encontram-se dentro do range de cores selecionados e guardamos as coordenadas dos azuis dentro do array data_corddata_cord = []height, width, channels = img.shapefor x in range(height): for y in range(width): r, g, b = img[x,y] if (r,g,b) &amp;gt;= BLUE_MIN and (r,g,b) &amp;lt;= BLUE_MAX: data_cord.append([int(x),int(y)])DBSCANAgora, com o data_cord já separado, iremos utilizar DBSCAN para clusterizar os dados.O DBSCAN é uma algoritmo de machine learning que clusteriza os dados com base em densidade e tamanho de cluster.X = StandardScaler().fit_transform(data_cord)db = DBSCAN(eps=0.1, min_samples=1).fit(X)core_samples_mask = np.zeros_like(db.labels_, dtype=bool)core_samples_mask[db.core_sample_indices_] = Truelabels = db.labels_Apresentação de clusters (Opcional)Este tópico não é obrigatório, mas caso queira visualizar os dados e verificar se está sendo separado corretamente.Utilizando Matplotlib, separamos os labels dos clusters e setamos uma cor para cada cluster.n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)unique_labels = set(labels)colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]for k, col in zip(unique_labels, colors): if k == -1: pass class_member_mask = (labels == k) xy = X[class_member_mask &amp;amp; core_samples_mask] plt.plot(xy[:, 0], xy[:, 1], &#39;o&#39;, markerfacecolor=tuple(col), markeredgecolor=&#39;k&#39;, markersize=14) xy = X[class_member_mask &amp;amp; ~core_samples_mask] plt.plot(xy[:, 0], xy[:, 1], &#39;o&#39;, markerfacecolor=tuple(col), markeredgecolor=&#39;k&#39;, markersize=6)plt.title(&#39;Estimated number of clusters: %d&#39; % n_clusters_)plt.show()Identificar maior clusterUtilizando o código abaixo, separamos os clusters em dict, verificamos qual o maior e salvamos essa informação em template_max.dict_data = dict(Counter(labels))template_max = max(dict_data, key=dict_data.get)ApresentaçãoNo código abaixo, realizamos a varredura dentro de labels e para cada vez que identificamos o maior cluster, pegamos este index e buscamos a mesma posição no data_cord.Assim, conseguimos pegar as coordenadas que se encontram no maior cluster e podemos manipular as coordenadas do maior cluster, ignorando o ruído geral da imagem.No caso abaixo, estamos pintando de vermelho o maior cluster e pintando de azul os demais(ruídos) apenas para fins demonstrativos.for i in range(len(labels)): if labels[i] == template_max: x,y = data_cord[i] img[x,y] = (0,0,255) else: x,y = data_cord[i] img[x,y] = (255,0,0)cv2_imshow(img)Atenciosamente&amp;lt;/br&amp;gt;Willian Jesus da Silva, Aluno do curso de Ciência da Computação no Instituto de ensino superior da Grande Florianópolis." }, { "title": "Processamento Morfológico de Imagens", "url": "/posts/cap2/", "categories": "Erosão, Dilatação, Morfologia", "tags": "", "date": "2020-12-04 08:25:34 -0300", "snippet": "Processamento Morfológico de ImagensImagens e ConjuntosEm algumas aplicações de Processamento Digital de Imagens podemos utilizar conceitos da Teoria dos conjuntos para realizarmos algumas análises e inferir certas informações em imagens. Por exemplo, podemos dizer que um objeto, representado em uma imagem, é formado pelo conjunto de pixels que o constituem. Figura 1: Pista de Corrida Na Figura 1 você pode perceber facilmente algumas árvores e uma pista de corrida. Podemos dizer que somos capazes de determinar visualmente as diferenças entre as árvores e a pista de corrida apenas afirmando que o conjunto dos pixels azuis formam a pista de corrida, enquanto o conjunto dos pixels de cor verde-escura formam as árvores.O que eu quero te mostrar neste capítulo é que podemos realizar algumas operações sobre estes conjuntos de pixels que representam objetos nas imagens! A estas operações sobre conjuntos de pixels chamamos de Processamento Morfológico de Imagens.Os Elementos EstruturantesJá sabemos que podemos considerar objetos representados em imagens como conjuntos e que podemos também realizar operações sobre estes conjuntos. Bom, mas se vamos fazer operações sobre este conjunto de pixels, precisamos de um outro conjunto para realizar estas operações, não é mesmo? Muito bem! Para realizarmos estas operações precisamos dos Elementos estruturantes!Observe a Figura 2, na qual temos uma imagem representando um objeto em tons de cinza que é imóvel, enquanto que, se movendo, temos um pequeno conjunto de pixels que está percorrendo esta imagem (algo parecido com um filtro). Figura 2: Erosão A Figura 2 mostra uma operação entre dois conjuntos: O elemento estruturante (filtro que se move ao longo da imagem) e o objeto representado na imagem. O resultado desta operação é uma redução da quantidade de pixels que presentam o objeto. Esta operação é chamada de Erosão.Como funciona a Erosão em uma imagem?Vamos considerar que o conjunto A é o objeto representado na imagem e que o conjunto B é o elemento estruturante, ambos apresentados na Figura 2. Primeiro é feita uma “varredura” do conjunto B (Elemento estruturante) em A para que a origem de B passe por todos os elementos de A. Depois é feita uma verificação: Para cada localização da origem de B, considera-se que o pixel de A é um membro do novo conjunto caso todos os elementos de B que são diferentes de zero estejam contidos em A, caso contrário, descarta-se este pixel. (No exemplo da imagem, a cor cinza representa o valor 1 e a cor branca representa o valor zero). Depois de fazer essa varedura em toda a imagem, o resultado final é alcançado com o objeto tendo um conjunto menor de pixels conforme mostra a Figura 2. Outra Operação: A Dilatação Figura 3: Dilatação Observe na Figura 3 o procedimento que está sendo representado: Temos o conjunto A (o objeto representado na imagem), o conjunto B (elemento estruturante) e novamente realizamos uma varredura de B em A. A diferença é que agora, ao contrário do processo de Erosão, o objeto representado na imagem não “perdeu” pixels, isto é, ao invés de diminuirmos o conjunto A, ele ficou ainda maior. Quando isto acontece dizemos que o conjunto A sofreu Dilatação.O que aconteceu neste caso é que modificamos a operação a ser realizada entre estes dois conjuntos para aplicar a Dilatação, assim podemos dizer que a dilatação é aplicada seguindo os seguintes passos: Faz-se uma “varredura” do conjunto B (Elemento estruturante) em A para que a origem de B passe por todos os elementos de A. Depois é feita uma verificação: Para cada localização da origem de B, considera-se que o pixel de A é um membro do novo conjunto se pelo menos um elemento de B esteja contido em A, caso contrário, descarta-se este pixel. Perceba que antes mesmo que a origem do elemento estruturante esteja contida dentro do conjunto A, o pixel abaixo dela já está contida dentro de A, portanto considera-se que a posição da origem do elemento estruturante é parte do novo conjunto A. Depois de fazer essa varedura em toda a imagem, o resultado final é alcançado com o objeto tendo um conjunto maior de pixels conforme mostra a Figura 3. Mão na massa com a OpenCV e PythonPré-requisitosO que será necessário para realizar os tutoriais a seguir: Python 3.x instalado em sua máquina; OpenCV 4; Um editor de código ou IDE de sua preferência (Eu utilizo o VS Code); Os dados que utilizaremos para executar os tutoriais podem ser baixados aqui.Exemplo 1: ErosãoObserve a imagem abaixo. Note que não há muitos detalhes nesta imagem e isso pode facilitar as coisas para nós. Figura 4: Exemplo 1 Primeiro vamos supor que, por algum motivo, queremos eliminar a linha que conecta os circulos. Esta é uma operação de remoção de alguns pixels, por isso usaremos a Erosão para remover os pixels desta linha.Com a OpenCV o processo de aplicar a erosão é muito simples:import cv2, sysimport numpy as np#lendo a imagemimg = cv2.imread(&quot;ex1.png&quot;,0)if img is None: print(&quot;Não foi possível ler a imagem!&quot;) sys.exit()#criando um elemento estruturante de tamanho 5x5kernel = np.ones((5,5),np.uint8)#Aplicando a erosãoerosao = cv2.erode(img, kernel, iterations = 1)cv2.imwrite(&quot;erosao_ex1.jpg&quot;, erosao)No código acima utilizamos um elemento estruturante (que também pode ser chamado de kernel) de tamanho 5x5 para aplicar a erosão. Neste kernel, todos os valores são iguais a 1. Você pode criar kernels personalizados que tenham também valores igual a zero, mas aqui utilizamos desta forma para facilitar.O parâmetro iterations é o número de vezes que a erosão será aplicada à imagem. No nosso caso, aplicamos a erosão apenas uma vez.E temos o seguinte resultado: Figura 5: Resultado da Erosão da Fig. 4 Sinta-se à vontade para brincar um pouco com os parâmetros deste trecho de código! Veja o que acontece quando o kernel tem tamanhos menores, tamanhos maiores e quando o número de iterações é maior!Podemos dizer que as principais aplicações para a erosão são: Remoção de ruídos na imagem; Remoção de atributos que não são interessantes para a aplicação em questão. Podemos imaginar que os círculos brancos são topos de postes e que a linha era um fio passando por eles. Através da erosão removemos o fio. Exemplo 2: DilataçãoVamos supor agora que temos uma imagem na qual os círculos tem alguns “buracos” e queremos fechá-los. Precisamos de uma operação que possa adiiconar pixels aos circulos brancos, logo, precisamos da Dilatação. Figura 6: Exemplo 2 E novamente podemos usar a openCV para implementar facilmente esta operação:import cv2, sysimport numpy as npimg = cv2.imread(&quot;ex2.png&quot;,0)if img is None: print(&quot;Não foi possível ler a imagem!&quot;) sys.exit()kernel = np.ones((5,5),np.uint8)dilatacao = cv2.dilate(img,kernel,iterations = 2)cv2.imwrite(&quot;dilatacao_ex2.jpg&quot;, dilatacao)Neste exemplo usamos o mesmo kernel que utilizamos no exemplo anterior e aplicamos a dilatação duas vezes (iterations = 2) para fechar os círculos brancos.Temos o seguinte resultado: Figura 7: Resultado da Dilatação da Fig. 6Te convido novamente a realizar testes alterando o tamanho do kernel e a quantidade de iterações para ver o que acontece em cada caso!Quero novamente chamar a sua atenção para o fato de que nós não apenas “fechamos os círculos”, mas note que os círculos estão maiores do que na imagem original (Fig. 4) e estão assumindo uma forma um pouco retangular, por isso devemos usar a dilatação com cuidado para que não modifiquemos demais as características de um objeto na imagem!Podemos dizer que as principais aplicações da Dilatação são: Quando queremos que objetos sejam “destacados” fazendo com que eles fiquem maiores na imagem; Quando queremos “fechar buracos” ou até mesmo conectar elementos que estão muito próximos um do outro na imagem, porém não o suficiente para se conectarem. AtenciosamenteNatália C. de AmorimMestre em Ciências Geodésicas e Doutoranda em Ciências Geodésicas na Universidade Federal do Paraná." }, { "title": "Detecção de Bordas", "url": "/posts/cap1/", "categories": "canny, bordas", "tags": "", "date": "2020-12-04 08:25:34 -0300", "snippet": "Detecção de BordasA visão computacional é uma área da ciência que desenvolve teorias e tecnologias como objetivos de extrair informações de dados multidimensionais. Quase sempre, recorremos a uma analogia de como nós detectamos e reconhecemos objetos. Um objeto é caracterizado por conjuntos de atributos como: cor, texturas e forma geométrica. Nesse sentido, a extração de contorno poder representar informações importantes sobre um determinado objeto. Por exemplo, podemos identificar diversas formas geométricas como retângulo circulo, triângulos, linhas e outros. Além do que os médoto de detecção não utilizam muito recurso computacional sendo uma técnica atraente para aplicação em sistemas embaracados.Nessa capítulo vamos conhecer a base dos algoritmos de detecção de borda e aplicar o algoritmo de Canny.DependênciasPara executar os scripts mostrado aqui, você precisará ter em sua máquina uma versão do python 3 e o OpenCV instalados. python3 OpenCVO que é uma borda?Uma borda á caracteriza por uma variação abrupta entre os pixels vizinhos de uma imagem. Figura 1: Pista de corrida Primeiro vamos analizar apenas na linha selecionada Figura 1, podemos representa-la por uma função I(x) cujo domínio é uma lista [254,254,173,138,79,44,45,53]Como nossa função é discreta (só admite valor inteiro) não podemos calcular diretamente a derivada dessa função mais podemos fazer uma boa aproximação.A derivada é uma operação matemática que permite calcular a taxa de variação de uma função ou de dois pontos muito próximos. Ela é definida pela equação 1. Equção 1: Derivada. Supondo que x seja a posição que estamos na lista, então f(x) é o valor do pixel e f(x+h) é próximo pixel. Acontece que, quando o intervalo h for muito pequeno vamos pegar variações decorreste de ruídos na imagem. sendo assim, não vamos preocupar em fazer pequenos ajustes nesse sentido. Por exemplo, podemo dizer que nossa derivada no ponto x é dada por f(x+h)-f(x-h), ou seja, a diferença do próximo pixel pelo pixel anterior ao ponto x. Equação 2: Derivada aproximada. Desconsideramos a divisão por h da Equação 1, porque nesse contexto ele é apenas um normalizador da função, ou seja, ele será um parâmetro que vamos passar ao realizar os cálculos. &amp;lt;Figura 2: Derivada aproximada para 1. Um dos motivos da aproximação de fizemos é por conta dos ruídos, porém essa nova equação pode ser representada por um kernel. Computacionalmente é mais interessante convolver um kernel por uma imagem do que aplicar uma função. Figura 3: Kernel para calculo de derivada. Na Figua 4 realizamos essa operação para toda a linha da imagem 1, tente identificar onde está a região que selecionamos. Figura 4: Grafico de linha da selecionada na figura 1. A lista começa com valor alto, 254 decai ate 44 e sobe novamente para 53. Essa variação acontece no intervalo 160 à 178 (aproximado) do eixo x.Se expandirmos esse ideia para um plano 2D nossa função anterior pode ser descrita da seguinte forma. Figura 5: Aproximação de derivada. Agora temos derivadas parciais. Da mesma forma, podemos rescrever isso por um kernel. Figura 6: Kernel para derivada parcial. Esse par de kernel na Figura 6 tem o nome de operador Sobel. O OpenCV tem esse operador implementado aqui cv2.sobel, então vamos usar.import cv2ddepth = cv2.CV_16S# Carrega imagen &quot;frame.png&quot; em escala de cinzagray = cv2.imread(&quot;frame.png&quot;,cv2.IMREAD_GRAYSCALE)# calcula derivada de primeira ordem na direção x grad_x = cv2.Sobel(gray, ddepth, 1, 0, ksize=3,scale = 1)# calcula derivada de primieira ordem na direção ygrad_y = cv2.Sobel(gray, ddepth, 0, 1, ksize=3,scale = 1)# calcula valor absoluto e converte para uint8 abs_grad_x = cv2.convertScaleAbs(grad_x)abs_grad_y = cv2.convertScaleAbs(grad_y)# Calcula gradientegrad = cv2.addWeighted(abs_grad_x, 0.5, abs_grad_y, 0.5, 0)# concatena image gerada com a originalsaida=cv2.hconcat((grad,gray))# redimenciona em 60%saida=cv2.resize(saida,None,None,0.4,0.4)#salva imagemcv2.imwrite(&quot;saida.png&quot;,saida)cv2.imshow(&quot;janela&quot;, saida)cv2.waitKey(0) Figura 7: Resultado do Sobel.Perceba que aplicamos o operador Sobel duas vezes, primeiro na direção x e depois na direção y. A composição dessas derivadas é matematicamente conhecida como gradiente. O gradiente é um vetor que aponta na direção onde a função tem a maior variação. No entanto, o que nos interessa aqui é magnitude desse gradiente, ou seja, o quão abrupta é essa variação. O módulo do gradiente poder ser calulado usando a Equação 2 (calculamos com a função cv2.addWeighted). Equação 3: Magnitude do gradiente. O Sobel é uma das operações mais relevantes para detectar contorno em imagens. Embora exista alternativas como cv2.Scharr que tem uma aproximação melhor da derivada. O Sobel ainda é um dos principais métodos empregados nos algoritmos para detecção de borda.Algoritmo de CannyO algoritmo de Canny executa vários estágio para detectar uma borda.1. remoção de ruídos.Na Figura 4, o gráfico da derivada apresenta bastante ruido, isso acontece porque pegamos micros variações locais. Canny usa um filtro gaussiano para resolver isso. Veja como o filtro afeta a derivada na Figura 8. Figura 8: Efeito de filtro gaussiano.2. Calcular gradientes.O filtro Sobel discutido no tópico anterior é usado aqui para calcular os gradientes.3. Máximos locais.Nessa etapa uma varredura completa é realizada na imagem em busca de gradientes máximos locais, esse processo elemina bordas largas ou duplicadas.4. Limiar de hesterese.Tudo que esta abaixo de minVal é descartado. o que esta entre minVal e maxVal é mantido apenas se parte do contorno estiver acima de maxVal. Na Figura 9, A é mantido porque esta acima de maxVal, C é mantido, embora esteja abaixo de maxVal ele esta conectado a A. Já o B é removido, pois esta totalmente dentro da área delimitada. Figura 9: Região delimitada pelos liminar maxVal e minVal.Fonte: https://docs.opencv.org/master/da/d22/tutorial_py_canny.htmlUsando CannyNo OpenCV temos uma implementação do algoritmo de Canny, o segundo e o terceiro parâmetros passados, são minVal e maxVal.import cv2# Carrega imagem em escala de cinzagray = cv2.imread(&#39;frame.png&#39;,cv2.IMREAD_GRAYSCALE)# aplica algoritmo de Canny com minVal=100 e maxVal=200edges = cv2.Canny(gray,100,200)# concatena imagem original com o resultadosaida=cv2.hconcat((gray,edges))#redimenciona saida=cv2.resize(saida,None,None,0.4,0.4)#Mostra saidacv2.imshow(&quot;janela&quot;,saida)cv2.waitKey() Figura 9: Resultado do Canny. Aqui deixo um vídeo com animação gráfica do que discutimos nesse artigo.ConclusãoDe fato, a área de visão computacional é permeada por aplicações matemáticas de alta complexidade. No entanto, bibliotecas como OpenCV tem simplificado, permitindo que pessoas de diversas áreas desenvolva suas própias aplicações. Se você gostou desse assunto, junte-se a nós no grupo opencvBrasil .AtenciosamenteElton fernandes dos SantosEngenheiro eletricista pela UNEMAT e mestrando em Zootecnia na Universidade Federal do Mato Grosso UFMT.Autor do blog visioncompyReferências Documentação oficial OpenCV v 4.5.0: Sobel. fonte https://docs.opencv.org/master/d2/d2c/tutorial_sobel_derivatives.html Documentação oficial OpenCV v 4.5.0: Algoritmo de Canny. fonte https://docs.opencv.org/master/da/d22/tutorial_py_canny.html " } ]
